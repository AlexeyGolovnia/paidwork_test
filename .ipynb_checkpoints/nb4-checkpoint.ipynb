{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ca525f-5ec3-4b7c-9259-2c2e4eed5cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Layout\n",
    "import cvzone\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.shared_memory import SharedMemory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import t2\n",
    "import pickle, dill\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    # else \"mps\"\n",
    "    # if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aec2e7-95ce-4953-b800-13f0dc51d61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7304eda-43f2-4857-a36b-9e8c6d7d209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "width_img = 1280\n",
    "height_img = 960\n",
    "\n",
    "a = [0,255,0,255,0,255]\n",
    "hue = widgets.IntRangeSlider(min=0, max=258, step=1, value=[a[0], a[1]], layout=Layout(width='80%'), description='Hue')\n",
    "sat = widgets.IntRangeSlider(min=0, max=255, step=1, value=[a[2], a[3]], layout=Layout(width='80%'), description='Saturation')\n",
    "val = widgets.IntRangeSlider(min=0, max=255, step=1, value=[a[4], a[5]], layout=Layout(width='80%'), description='Value')\n",
    "\n",
    "x1 = widgets.IntSlider(min=0, max=width_img, step=1, value=100, layout=Layout(width='80%'), description='x1')\n",
    "y1 = widgets.IntSlider(min=0, max=height_img, step=1, value=100, layout=Layout(width='80%'), description='y1')\n",
    "w = widgets.IntSlider(min=0, max=width_img, step=1, value=300, layout=Layout(width='80%'), description='w')\n",
    "h = widgets.IntSlider(min=0, max=height_img, step=1, value=300, layout=Layout(width='80%'), description='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dfce3108-d757-48fe-b5e2-65ff8efa3ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45099bdc930946808e1bc7a39e1bda52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(ToggleButton(value=False, description='Stop'), HTML(value=' ', description='Train:')), layouâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = mp.Value('i', 0)\n",
    "contours_size = mp.Value('i', 0)\n",
    "len_contours = mp.Value('i', 0)\n",
    "\n",
    "frame_memory = SharedMemory(name='FrameMemory', create=True, size=width_img*height_img*3)\n",
    "contours_memory = SharedMemory(name='ContoursMemory', create=True, size=4096)\n",
    "\n",
    "p1 = mp.Process(target=t2.my_process, args=(tmp, width_img, height_img, len_contours, contours_size,), daemon=True)\n",
    "p1.start()\n",
    "\n",
    "stopButton = widgets.ToggleButton(description='Stop', disabled=False)\n",
    "output = widgets.HTML(value=' ', description='Train:', disabled=False)\n",
    "grid = widgets.GridBox([stopButton, output], layout=widgets.Layout(grid_template_columns='repeat(2, 200px)'))\n",
    "\n",
    "def preprocess(fr):\n",
    "    mask = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
    "    mask = cv2.GaussianBlur(mask, (5,5), 2)\n",
    "    mask = cv2.Canny(mask, 30, 255, apertureSize=5)\n",
    "    return mask\n",
    "\n",
    "def contours(fr_m, fr):\n",
    "    biggest = np.array([])\n",
    "    max_area = 0\n",
    "    contours, hierarchy = cv2.findContours(fr_m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 100_000:\n",
    "            epsilon = 0.1 * cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            cv2.drawContours(fr, cnt, -1, (0,255,0), 5)\n",
    "            if area > max_area and len(approx) == 4:\n",
    "                biggest = approx\n",
    "                max_area = area\n",
    "    return biggest\n",
    "\n",
    "def reorder(my_points):\n",
    "    my_points = my_points.reshape((4,2))\n",
    "    my_points_new = np.zeros((4,1,2), np.int32)\n",
    "    add = my_points.sum(1)\n",
    "\n",
    "    my_points_new[0] = my_points[np.argmin(add)]\n",
    "    my_points_new[3] = my_points[np.argmax(add)]\n",
    "\n",
    "    diff = np.diff(my_points, axis=1)\n",
    "\n",
    "    my_points_new[1] = my_points[np.argmin(diff)]\n",
    "    my_points_new[2] = my_points[np.argmax(diff)]\n",
    "\n",
    "    return my_points_new\n",
    "\n",
    "def get_warp(img, approx):\n",
    "    biggest = reorder(approx)\n",
    "\n",
    "    pts1 = np.float32(biggest)\n",
    "    pts2 = np.float32([[0, 0], [width_img, 0], [0, height_img], [width_img, height_img]])\n",
    "    matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    img_output = cv2.warpPerspective(img, matrix, (width_img, height_img))\n",
    "    img_cropped = cv2.resize(img_output, (width_img, height_img))\n",
    "    image_grayworld = (img_cropped * (img_cropped.mean() / img_cropped.mean(axis=(0, 1)))).astype(np.uint8)\n",
    "    return image_grayworld\n",
    "\n",
    "# ID_card TRAIN\n",
    "def text_detection(img, contours_data):\n",
    "    tensor = np.empty((1,4))\n",
    "    for contour in contours_data[:,:]:\n",
    "        x, y, w, h = contour[0], contour[2], contour[1], contour[3]\n",
    "        cv2.rectangle(img, (x, y), (w, h), (255, 255, 0), 6)\n",
    "        tensor = np.append(tensor, [[x, y, w, h]], axis=0)\n",
    "    if len_contours.value > 1:\n",
    "        m = int(184/4)\n",
    "        t = tensor[1:m,:].astype(int)\n",
    "        t = np.pad(t, [(0, 46 - t.shape[0]),(0, 0)], mode='constant') # 184/4\n",
    "        t = t.T.reshape(1, -1)\n",
    "        x11 = torch.tensor(t).float().to(device)\n",
    "        prediction = model(x11)\n",
    "        a = float(torch.softmax(prediction, dim=1)[0][0])\n",
    "        a = str(round(a, 3))\n",
    "        b = float(torch.softmax(prediction, dim=1)[0][1])\n",
    "        b = str(round(b, 3))\n",
    "        c = float(torch.softmax(prediction, dim=1)[0][2])\n",
    "        c = str(round(c, 3))\n",
    "        d = float(torch.softmax(prediction, dim=1)[0][3])\n",
    "        d = str(round(d, 3))\n",
    "        output.value = a + '/' + b + '/' + c + '/' + d\n",
    "        \n",
    "        \n",
    "        # if len(l2) < 400:   \n",
    "        #     l2.append((tensor[1:,:].astype(int), 3))\n",
    "        # else:\n",
    "        #     output.value = 'complete'\n",
    "\n",
    "# ID_card CHECK\n",
    "# def text_detection(img, contours_data):\n",
    "#     t = np.empty((1,4))\n",
    "#     for contour in contours_data[:,:]:\n",
    "#         x, y, w, h = contour[0], contour[2], contour[1], contour[3]\n",
    "#         cv2.rectangle(img, (x, y), (w, h), (255, 255, 0), 6)\n",
    "#         t = np.append(t, [[x, y, w, h]], axis=0)\n",
    "    \n",
    "#     if len_contours.value > 1:\n",
    "#         t = t[1:,:] # 1:13\n",
    "#         # t1 = np.pad(t, [(0, 12 - t.shape[0]),(0, 0)], mode='constant') # s=12\n",
    "#         # x = torch.tensor(np.reshape(t1, (1,t1.shape[0]*4))).float().to(device)\n",
    "#         # prediction = model(x)\n",
    "#         # a = float(torch.softmax(prediction, dim=1)[0][0])\n",
    "#         # a = str(round(a, 3))\n",
    "#         # b = float(torch.softmax(prediction, dim=1)[0][1])\n",
    "#         # b = str(round(b, 3))\n",
    "#         # output.value = a + '/' + b\n",
    "        \n",
    "#         if len(l2) < 100:   \n",
    "#             l2.append((t.astype(int), 0))\n",
    "#         else:\n",
    "#             output.value = 'complete'\n",
    "                \n",
    "\n",
    "def close(cap):\n",
    "    cap.release()\n",
    "    frame_memory.close()\n",
    "    frame_memory.unlink()\n",
    "    contours_memory.close()\n",
    "    contours_memory.unlink()\n",
    "    p1.terminate()\n",
    "\n",
    "def view(grid):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    frame_data = np.ndarray((height_img,width_img,3), dtype=np.uint8, buffer=frame_memory.buf)\n",
    "    contours_data = np.ndarray((1,4), dtype=np.int32, buffer=contours_memory.buf)\n",
    "\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        frame_masked = preprocess(frame)\n",
    "        frame_contours = contours(frame_masked, frame)\n",
    "\n",
    "        if frame_contours.size != 0:\n",
    "            img_warped = get_warp(frame, frame_contours)\n",
    "            contours_size.value = 1\n",
    "            if tmp.value == 1:\n",
    "                frame_data[:,:,:] = img_warped[:,:,:]\n",
    "                contours_data = np.ndarray((len_contours.value,4), dtype=np.int32, buffer=contours_memory.buf)\n",
    "                tmp.value = 0\n",
    "            frame_detect = text_detection(img_warped, contours_data)\n",
    "        else:\n",
    "            img_warped = frame.copy()\n",
    "            len_contours.value = 0\n",
    "            frame_data[:,:,:] = img_warped[0,0,0]\n",
    "\n",
    "        img_stack = cvzone.stackImages([\n",
    "            frame,\n",
    "            img_warped,\n",
    "        ], 2, 1)\n",
    "\n",
    "        _, frame = cv2.imencode('.jpeg', img_stack) \n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        if stopButton.value==True:\n",
    "            close(cap)\n",
    "            display_handle.update(None)\n",
    "\n",
    "display(grid)\n",
    "thread = threading.Thread(target=view, args=(grid,))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad03e1d5-3969-4167-936a-84e246c2d617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.terminate()\n",
    "frame_memory.close()\n",
    "contours_memory.close()\n",
    "frame_memory.unlink()\n",
    "contours_memory.unlink()\n",
    "time.sleep(0.1)\n",
    "p1.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1df1e5c3-825c-42c4-8556-b0b042834d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(thread.is_alive(), p1.is_alive())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e68f7-8dd7-4a4b-b4ef-698184ffec34",
   "metadata": {},
   "source": [
    "## -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55030edb-a1a6-4865-9e92-363bdf81a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9480ef59-fba3-4e5d-b51e-86349b66a6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0e3aa8-2102-49dc-8104-af969b14bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.zeros(48).reshape(-1,4)\n",
    "# a\n",
    "# for _ in range(100):\n",
    "#     l2.append((a.astype(int), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03eaa1ff-9cc9-48b3-8418-1de8e58c599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  24,   34,  394,   77],\n",
       "        [  21,   65,  399,  126],\n",
       "        [  26,  198,  271,  239],\n",
       "        [  23,  227,  401,  288],\n",
       "        [  24,  360, 1260,  434],\n",
       "        [  26,  431, 1261,  497],\n",
       "        [  26,  475, 1265,  542],\n",
       "        [  49,  621, 1240,  725],\n",
       "        [  49,  702, 1238,  802],\n",
       "        [  56,  787, 1243,  878]]),\n",
       " 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2[121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c6967d-a072-447b-9e35-2e35dfddf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(itemlist):\n",
    "    s = 0\n",
    "    # len_dict = len(target_dict)\n",
    "    len_dict = 2\n",
    "    \n",
    "    for i in itemlist:\n",
    "        if s < i[0].shape[0]:\n",
    "            s = i[0].shape[0]\n",
    "            \n",
    "    ret_input = np.empty((1, s * 4))\n",
    "    # ret_target = np.empty((1, len_dict))\n",
    "    # ret_target = np.empty((1, 1))\n",
    "    # ret_target = np.empty((1))\n",
    "    ret_target = []\n",
    "    eye = np.eye(len_dict)\n",
    "            \n",
    "    for i in itemlist:\n",
    "        t = np.pad(i[0], [(0, s - i[0].shape[0]),(0, 0)], mode='constant')\n",
    "        \n",
    "        ret_input = np.vstack((ret_input, t.T.reshape(1, -1)))\n",
    "        # ret_target = np.vstack((ret_target , eye[i[1]]))\n",
    "        # ret_target = np.vstack((ret_target , i[1]))\n",
    "        ret_target.append(i[1])\n",
    "\n",
    "        \n",
    "    # return ret_input[1:,:], ret_target[1:,:], s\n",
    "    return ret_input[1:,:], ret_target, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6853bb-186c-40ae-b5e1-f74a42d0f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = data_prepare(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67c3581e-6e63-412e-ba43-e7ba7619a73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24.,   21.,   26.,   23.,   24.,   26.,   26.,   49.,   49.,\n",
       "         56.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,   34.,   65.,  198.,  227.,  360.,  431.,  475.,  621.,\n",
       "        702.,  787.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,  394.,  399.,  271.,  401., 1260., 1261., 1265.,\n",
       "       1240., 1238., 1243.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,   77.,  126.,  239.,  288.,  434.,  497.,\n",
       "        542.,  725.,  802.,  878.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[0][121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bb25e7c-3a93-4a87-a12e-4762dfdca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pr[0], pr[1], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98076167-1b65-4522-8cc7-3a8347ba050a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320, 184), (80, 184), 320, 80)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12551bde-c4da-4137-87af-48f704a89d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pr[0]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "003d5d40-5970-4d6a-8132-ebd794246a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  18.,   19.,   24.,   21.,   23.,   25.,   24.,   47.,   49.,\n",
       "         51.,    0.,    0.,   34.,   68.,  201.,  231.,  361.,  428.,\n",
       "        474.,  621.,  703.,  781.,    0.,    0.,  394.,  397.,  270.,\n",
       "        399., 1258., 1260., 1261., 1235., 1232., 1240.,    0.,    0.,\n",
       "         82.,  130.,  243.,  294.,  439.,  503.,  548.,  729.,  805.,\n",
       "        883.,    0.,    0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77ea64-2ce6-4ed4-a032-b9a86037ece9",
   "metadata": {},
   "source": [
    "## ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db35b218-0064-4c1f-848e-3888eaf9f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyNet, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(200, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(24, 12),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(12, 6),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(6, 1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccef7fa6-a8fe-4eae-b642-92053f5902e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_ID(nn.Module):\n",
    "    def __init__(self, in_layers, out_layers):\n",
    "        super(Model_ID, self).__init__()\n",
    "        self.start = nn.Sequential(\n",
    "            nn.Linear(in_layers, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.out = nn.Linear(512, out_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.out(x)\n",
    "        # return torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8cc11d54-6a8d-4504-96f2-b0b3d077bfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9291f295-6993-4a43-a8db-3400ff153b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_ID(pr[0].shape[1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8642da01-5c82-4caf-a056-9d6dbe4ae032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_ID(\n",
       "  (start): Sequential(\n",
       "    (0): Linear(in_features=184, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8caa6128-921c-4f11-a556-48faf274af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, sh):\n",
    "        # self.X = torch.tensor(X.astype(np.float32))\n",
    "        # self.y = torch.tensor(np.array(y).astype(np.float32))\n",
    "        self.X = torch.tensor(X.astype(np.float32))\n",
    "        self.y = torch.tensor(np.array(y)).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        # im = self.X[x]\n",
    "        # im = np.reshape(im, (1, 1, sh))\n",
    "        return self.X[x].to(device), self.y[x].to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc8141e6-a573-40b4-b612-6692894ee083",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(X_train, y_train, pr[0].shape[1])\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_data = MyDataset(X_test, y_test, pr[0].shape[1])\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8db49952-dbcc-4fb4-be9b-4013869a9c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dataloader.dataset.y[320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930faab-b8e8-4a12-9518-6ba5b6698c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3033118e-a617-48fe-9728-642e7232520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch 1\n",
      "tr_loss 35.839354817413174\n",
      "tr_acc 0.990625\n",
      "val_loss 0.7127502048388121\n",
      "val_acc 0.9625\n",
      "   epoch 2\n",
      "tr_loss 11.476247092783378\n",
      "tr_acc 1.0\n",
      "val_loss 0.00036164413900223734\n",
      "val_acc 1.0\n"
     ]
    }
   ],
   "source": [
    "def train_batch(x, y, model, opt, loss_fn):\n",
    "    model.train()\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_fn(prediction, y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return batch_loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(x, y, model):\n",
    "    model.eval()\n",
    "    prediction = model(x)\n",
    "    is_correct = (prediction.argmax(1) == y)\n",
    "    return is_correct.cpu().numpy().tolist()\n",
    "\n",
    "def val_batch(x, y, model, opt, loss_fn):\n",
    "    model.eval()\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_fn(prediction, y)\n",
    "    return batch_loss.item()\n",
    "\n",
    "model = Model_ID(pr[0].shape[1],4).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr= 1e-5)\n",
    "\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "for epoch in range(2):\n",
    "    print(f\"   epoch {epoch + 1}\")\n",
    "    train_epoch_losses, train_epoch_accuracies = [], []\n",
    "    val_epoch_losses, val_epoch_accuracies = [], []\n",
    "\n",
    "    for ix, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "        train_epoch_losses.append(batch_loss)\n",
    "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
    "\n",
    "    for ix, batch in enumerate(iter(train_dataloader)):\n",
    "        x, y = batch\n",
    "        is_correct = accuracy(x, y, model)\n",
    "        train_epoch_accuracies.extend(is_correct)\n",
    "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "    \n",
    "    for ix, batch in enumerate(iter(test_dataloader)):\n",
    "        x, y = batch\n",
    "    \n",
    "        batch_loss = val_batch(x, y, model, optimizer, loss_fn)\n",
    "        val_epoch_losses.append(batch_loss)\n",
    "        \n",
    "        val_is_correct = accuracy(x, y, model)\n",
    "        val_epoch_accuracies.extend(val_is_correct)\n",
    "\n",
    "    # for ix, batch in enumerate(model_dataloader):\n",
    "    #     x, y = batch\n",
    "\n",
    "    #     batch_loss = val_batch(x, y, model, optimizer, loss_fn)\n",
    "    #     val_epoch_losses.append(batch_loss)\n",
    "        \n",
    "    #     is_correct = accuracy(x, y, model)\n",
    "    #     train_epoch_accuracies.extend(is_correct)\n",
    "    # train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "\n",
    "    val_epoch_loss = np.array(val_epoch_losses).mean()\n",
    "    val_epoch_accuracy = np.mean(val_epoch_accuracies)\n",
    "    \n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracies.append(train_epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    print('tr_loss',train_epoch_loss)\n",
    "    print('tr_acc',train_epoch_accuracy)\n",
    "    print('val_loss',val_epoch_loss)\n",
    "    print('val_acc',val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32c143f4-66a6-4b96-a56e-18877e647189",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'temp_model_0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f43df-53db-4e74-8939-eea0c6d7feaf",
   "metadata": {},
   "source": [
    "## -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6d8cb404-f9c5-474d-a76a-52c47d4d0e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_ID(\n",
       "  (start): Sequential(\n",
       "    (0): Linear(in_features=184, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (hidden): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (out): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_ID(pr[0].shape[1],4).to(device)\n",
    "model.load_state_dict(torch.load('temp_model_0.pth', map_location=torch.device('cpu')))\n",
    "# model = model.load_state_dict(torch.load('temp_model_0.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa014ed2-64bb-4218-bb5b-105bbc61f5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  24.   21.   26.   23.   24.   26.   26.   49.   49.   56.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   34.   65.\n",
      "  198.  227.  360.  431.  475.  621.  702.  787.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.  394.  399.  271.  401.\n",
      " 1260. 1261. 1265. 1240. 1238. 1243.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.   77.  126.  239.  288.  434.  497.\n",
      "  542.  725.  802.  878.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.]\n",
      "tensor([[  24.,   21.,   26.,   23.,   24.,   26.,   26.,   49.,   49.,   56.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,   34.,   65.,  198.,  227.,\n",
      "          360.,  431.,  475.,  621.,  702.,  787.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,  394.,  399.,  271.,  401., 1260., 1261., 1265., 1240.,\n",
      "         1238., 1243.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,   77.,  126.,\n",
      "          239.,  288.,  434.,  497.,  542.,  725.,  802.,  878.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "            0.,    0.,    0.,    0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([8.6906e-41, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = pr[0][0]\n",
    "x = np.zeros(48)\n",
    "x = pr[0][121]\n",
    "print(x)\n",
    "x = torch.tensor(np.reshape(x, (1,x.shape[0]))).float().to(device)\n",
    "print(x)\n",
    "prediction = model(x)\n",
    "torch.softmax(prediction, dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b7c9038-bd3c-4633-8831-d3ff8cdd03ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3162335157394409"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(torch.softmax(prediction, dim=1)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "68ea828b-ce98-43a9-a3b5-d2fbaea85151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41.,   40.,   38.,   35.,   28.,   27.,   26., 1110.,   42.,\n",
       "         39.,   41.,    0.,   15.,   50.,  178.,  212.,  359.,  426.,\n",
       "        472.,  499.,  624.,  701.,  784.,    0.,  416.,  418.,  285.,\n",
       "        414., 1265., 1262., 1114., 1261., 1231., 1227., 1228.,    0.,\n",
       "         56.,  102.,  215.,  268.,  443.,  507.,  547.,  550.,  729.,\n",
       "        810.,  887.,    0.])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4c6691be-4f58-47e8-a7dd-7aa235b8d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34052352-44f1-42a5-9d36-8fca95aab431",
   "metadata": {},
   "source": [
    "## -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fee3d2-f3a2-4f56-a1b5-cd7e770cf27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.start = nn.Linear(500, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a651024e-ae8a-48bc-b3fe-9a1f10927884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e00db1-09d9-4cc0-abd1-3beb2a54c221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
